{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Keras exercise\n\nIn this exercise you will be creating a Keras model by loading a data set, preprocessing input data, building a Sequential Keras model and compiling the model with a training configuration. Afterwards, you train your model on the training data and evaluate it on the test set. To finish this exercise, you will past the accuracy of your model to the Coursera grader.\n\nThis notebook is tested in IBM Watson Studio under python 3.6\n\n##\u00a0Data\n\nFor this exercise we will use the Reuters newswire dataset. This dataset consists of 11,228 newswires from the Reuters news agency. Each wire is encoded as a sequence of word indexes, just as in the IMDB data we encountered in lecture 5 of this series. Moreover, each wire is categorised into one of 46 topics, which will serve as our label. This dataset is available through the Keras API.\n\n## Goal\n\nWe want to create a Multi-layer perceptron (MLP) using Keras which we can train to classify news items into the specified 46 topics.\n\n## Instructions\n\nWe start by installing and importing everything we need for this exercise:"}, {"metadata": {}, "cell_type": "code", "source": "!pip install tensorflow==2.2.0rc0", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200318055615-0005\nKERNEL_ID = 8dba757e-308a-441e-8349-a841b135b0c8\nCollecting tensorflow==2.2.0rc0\n  Using cached https://files.pythonhosted.org/packages/c2/a9/836bf28d748b12e8e979fb24a0d9d2f0930df913eb958363ed223406a0ca/tensorflow-2.2.0rc0-cp36-cp36m-manylinux2010_x86_64.whl\nCollecting astunparse==1.6.3 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\nCollecting h5py<2.11.0,>=2.10.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl\nCollecting scipy==1.4.1; python_version >= \"3\" (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl\nCollecting google-pasta>=0.1.8 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\nCollecting opt-einsum>=2.3.2 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/b2/49/2233e63052d5686c72131b579837ddfb98ba9dd0b92bb91efcb441ada8ce/opt_einsum-3.2.0-py3-none-any.whl\nCollecting protobuf>=3.8.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl\nCollecting absl-py>=0.7.0 (from tensorflow==2.2.0rc0)\nCollecting wheel>=0.26; python_version >= \"3\" (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\nCollecting gast==0.3.3 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\nCollecting termcolor>=1.1.0 (from tensorflow==2.2.0rc0)\nCollecting tensorboard<2.2.0,>=2.1.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/d9/41/bbf49b61370e4f4d245d4c6051dfb6db80cec672605c91b1652ac8cc3d38/tensorboard-2.1.1-py3-none-any.whl\nCollecting numpy<2.0,>=1.16.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/07/08/a549ba8b061005bb629b76adc000f3caaaf881028b963c2e18f811c6edc1/numpy-1.18.2-cp36-cp36m-manylinux1_x86_64.whl\nCollecting tensorflow-estimator<2.2.0,>=2.1.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl\nCollecting keras-preprocessing>=1.1.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\nCollecting wrapt>=1.11.1 (from tensorflow==2.2.0rc0)\nCollecting six>=1.12.0 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/65/eb/1f97cb97bfc2390a276969c6fae16075da282f5058082d4cb10c6c5c1dba/six-1.14.0-py2.py3-none-any.whl\nCollecting grpcio>=1.8.6 (from tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/28/df/1f8a284a5e5819ae07d50bd76996d6f7208afef7533e4896fa1c6445574f/grpcio-1.27.2-cp36-cp36m-manylinux2010_x86_64.whl\nCollecting setuptools (from protobuf>=3.8.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/70/b8/b23170ddda9f07c3444d49accde49f2b92f97bb2f2ebc312618ef12e4bd6/setuptools-46.0.0-py3-none-any.whl\nCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\nCollecting google-auth<2,>=1.6.3 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl\nCollecting requests<3,>=2.21.0 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/1a/70/1935c770cb3be6e3a8b78ced23d7e0f3b187f5cbfab4749523ed65d7c9b1/requests-2.23.0-py2.py3-none-any.whl\nCollecting werkzeug>=0.11.15 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/ba/a5/d6f8a6e71f15364d35678a4ec8a0186f980b3bd2545f40ad51dd26a87fb1/Werkzeug-1.0.0-py2.py3-none-any.whl\nCollecting markdown>=2.6.8 (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/ab/c4/ba46d44855e6eb1770a12edace5a165a0c6de13349f592b9036257f3c3d3/Markdown-3.2.1-py2.py3-none-any.whl\nCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\nCollecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\nCollecting rsa<4.1,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\nCollecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl\nCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/e8/74/6e4f91745020f967d09332bb2b8b9b10090957334692eb88ea4afe91b77f/urllib3-1.25.8-py2.py3-none-any.whl\nCollecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/89/e3/afebe61c546d18fb1709a61bee788254b40e736cff7271c7de5de2dc4128/idna-2.9-py2.py3-none-any.whl\nCollecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\nCollecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl\nCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl\nCollecting pyasn1>=0.1.3 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.2.0rc0)\n  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n\u001b[31mibm-cos-sdk-core 2.4.3 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.8 which is incompatible.\u001b[0m\n\u001b[31mbotocore 1.12.82 has requirement urllib3<1.25,>=1.20, but you'll have urllib3 1.25.8 which is incompatible.\u001b[0m\nInstalling collected packages: wheel, six, astunparse, numpy, h5py, scipy, google-pasta, opt-einsum, setuptools, protobuf, absl-py, gast, termcolor, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, oauthlib, urllib3, idna, chardet, certifi, requests, requests-oauthlib, google-auth-oauthlib, grpcio, werkzeug, markdown, tensorboard, tensorflow-estimator, keras-preprocessing, wrapt, tensorflow\n", "name": "stdout"}, {"output_type": "stream", "text": "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.0.0 certifi-2019.11.28 chardet-3.0.4 gast-0.3.3 google-auth-1.11.3 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.27.2 h5py-2.10.0 idna-2.9 keras-preprocessing-1.1.0 markdown-3.2.1 numpy-1.18.2 oauthlib-3.1.0 opt-einsum-3.2.0 protobuf-3.11.3 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.23.0 requests-oauthlib-1.3.0 rsa-4.0 scipy-1.4.1 setuptools-46.0.0 six-1.14.0 tensorboard-2.1.1 tensorflow-2.2.0rc0 tensorflow-estimator-2.1.0 termcolor-1.1.0 urllib3-1.25.8 werkzeug-1.0.0 wheel-0.34.2 wrapt-1.12.1\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tensorboard-2.1.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/wheel already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/easy_install.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six-1.14.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/oauthlib already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/werkzeug already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/gast already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1_modules already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet-3.0.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/Keras_Preprocessing-1.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/termcolor-1.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/google already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/rsa already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/astunparse-1.6.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1-0.4.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/google_auth-1.11.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/opt_einsum-3.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/oauthlib-3.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tensorflow-2.2.0rc0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tensorflow_estimator-2.1.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pyasn1_modules-0.2.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/chardet already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/grpcio-1.27.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/h5py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/grpc already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/google_pasta-0.2.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/protobuf-3.11.3-py3.6-nspkg.pth already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3-1.25.8.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/certifi-2019.11.28.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna-2.9.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/gast-0.3.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/absl already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pasta already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/google_auth_oauthlib already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/wrapt already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/pkg_resources already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/wheel-0.34.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tensorflow_estimator already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/markdown already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests-2.23.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/rsa-4.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/google_auth-1.11.3-py3.8-nspkg.pth already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy-1.18.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scipy-1.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/setuptools-46.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests_oauthlib-1.3.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/Markdown-3.2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/wrapt-1.12.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tensorflow already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/h5py-2.10.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/astunparse already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/idna already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/protobuf-3.11.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cachetools already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/google_auth_oauthlib-0.4.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/urllib3 already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/termcolor.py already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/tensorboard already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/setuptools already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests_oauthlib already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/cachetools-4.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/absl_py-0.9.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/scipy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/Werkzeug-1.0.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/keras_preprocessing already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/numpy already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/opt_einsum already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/requests already exists. Specify --upgrade to force replacement.\u001b[0m\n\u001b[33mTarget directory /home/spark/shared/user-libs/python3.6/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "import tensorflow as tf\nif not tf.__version__ == '2.2.0-rc0':\n    print(tf.__version__)\n    raise ValueError('please upgrade to TensorFlow 2.2.0-rc0, or restart your Kernel (Kernel->Restart & Clear Output)')", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "IMPORTANT! => Please restart the kernel by clicking on \"Kernel\"->\"Restart and Clear Outout\" and wait until all output disapears. Then your changes are beeing picked up\n\nAs you can see, we use Keras' Sequential model with only two types of layers: Dense and Dropout. We also specify a random seed to make our results reproducible. Next, we load the Reuters data set:"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nseed = 1337\nnp.random.seed(seed)\nfrom tensorflow.keras.datasets import reuters\n\nmax_words = 1000\n(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n                                                         test_split=0.2,\n                                                         seed=seed)\nnum_classes = np.max(y_train) + 1  # 46 topics", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Note that we cap the maximum number of words in a news item to 1000 by specifying the *num_words* key word. Also, 20% of the data will be test data and we ensure reproducibility by setting our random seed.\n\nOur training features are still simply sequences of indexes and we need to further preprocess them, so that we can plug them into a *Dense* layer. For this we use a *Tokenizer* from Keras' text preprocessing module. This tokenizer will take an index sequence and map it to a vector of length *max_words=1000*. Each of the 1000 vector positions corresponds to one of the words in our newswire corpus. The output of the tokenizer has a 1 at the i-th position of the vector, if the word corresponding to i is in the description of the newswire, and 0 otherwise. Even if this word appears multiple times, we still just put a 1 into our vector, i.e. our tokenizer is binary. We use this tokenizer to transform both train and test features:"}, {"metadata": {}, "cell_type": "code", "source": "from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=max_words)\nx_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\nx_test = tokenizer.sequences_to_matrix(x_test, mode='binary')", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Exercise part: label encoding\n\nUse to_categorical, as we did in the lectures, to transform both *y_train* and *y_test* into one-hot encoded vectors of length *num_classes*:"}, {"metadata": {}, "cell_type": "code", "source": "y_train = to_categorical(y_train, num_classes)\ny_test = to_categorical(y_test, num_classes)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "##\u00a02. Exercise part: model definition\n\nNext, initialise a Keras *Sequential* model and add three layers to it:\n\n    Layer: Add a *Dense* layer with in input_shape=(max_words,), 512 output units and \"relu\" activation.\n    Layer: Add a *Dropout* layer with dropout rate of 50%.\n    Layer: Add a *Dense* layer with num_classes output units and \"softmax\" activation."}, {"metadata": {}, "cell_type": "code", "source": "model = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,), activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Exercise part: model compilation\n\nAs the next step, we need to compile our Keras model with a training configuration. Compile your model with \"categorical_crossentropy\" as loss function, \"adam\" as optimizer and specify \"accuracy\" as evaluation metric. NOTE: In case you get an error regarding h5py, just restart the kernel and start from scratch"}, {"metadata": {}, "cell_type": "code", "source": "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Exercise part: model training and evaluation\n\nNext, define the batch_size for training as 32 and train the model for 5 epochs on *x_train* and *y_train* by using the *fit* method of your model. Then calculate the score for your trained model by running *evaluate* on *x_test* and *y_test* with the same batch size as used in *fit*."}, {"metadata": {}, "cell_type": "code", "source": "batch_size = 32\nmodel.fit(x_train, y_train, batch_size=batch_size, epochs=5, validation_data=(x_test, y_test))\nscore = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0 )", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "Epoch 1/5\n281/281 [==============================] - 1s 5ms/step - loss: 1.3972 - accuracy: 0.6855 - val_loss: 0.9713 - val_accuracy: 0.7836\nEpoch 2/5\n281/281 [==============================] - 1s 4ms/step - loss: 0.7716 - accuracy: 0.8187 - val_loss: 0.8331 - val_accuracy: 0.8037\nEpoch 3/5\n281/281 [==============================] - 1s 4ms/step - loss: 0.5456 - accuracy: 0.8665 - val_loss: 0.7953 - val_accuracy: 0.8103\nEpoch 4/5\n281/281 [==============================] - 1s 4ms/step - loss: 0.4195 - accuracy: 0.8971 - val_loss: 0.8161 - val_accuracy: 0.8023\nEpoch 5/5\n281/281 [==============================] - 1s 4ms/step - loss: 0.3396 - accuracy: 0.9128 - val_loss: 0.8362 - val_accuracy: 0.8032\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "If you have done everything as specified, in particular set the random seed as we did above, your test accuracy should be around 80% "}, {"metadata": {}, "cell_type": "code", "source": "score[1]", "execution_count": 9, "outputs": [{"output_type": "execute_result", "execution_count": 9, "data": {"text/plain": "0.8032057285308838"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Congratulations, now it's time to submit your result to the Coursera grader by executing the following cells (Programming Assingment, Week2). \n\nWe have to install a little library in order to submit to coursera\n"}, {"metadata": {}, "cell_type": "code", "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "--2020-03-18 05:57:15--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.16.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.16.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2540 (2.5K) [text/plain]\nSaving to: 'rklib.py'\n\n100%[======================================>] 2,540       --.-K/s   in 0s      \n\n2020-03-18 05:57:15 (39.0 MB/s) - 'rklib.py' saved [2540/2540]\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Please provide your email address and obtain a submission token (secret) on the grader\u2019s submission page in coursera, then execute the cell"}, {"metadata": {}, "cell_type": "code", "source": "from rklib import submit\nimport json\n\nkey = \"XbAMqtjdEeepUgo7OOVwng\"\npart = \"HCvcp\"\nemail = 'jkarock3@gmail.com'\ntoken = 'pQnogx7FpIqOHIvH' #you can obtain it from the grader page on Coursera (have a look here if you need more information on how to obtain the token https://youtu.be/GcDo0Rwe06U?t=276)\n\n\nsubmit(email, token, 'XbAMqtjdEeepUgo7OOVwng', part, [part], json.dumps(score[1]*100))", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "Submission successful, please check on the coursera grader page for the status\n-------------------------\n{\"elements\":[{\"itemId\":\"ozVf2\",\"id\":\"tE4j0qhMEeecqgpT6QjMdA~ozVf2~UfRXN2jdEeqPGBJY6YdUNQ\",\"courseId\":\"tE4j0qhMEeecqgpT6QjMdA\"}],\"paging\":{},\"linked\":{}}\n-------------------------\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}